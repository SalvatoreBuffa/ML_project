{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Fase di import"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/s4lv0/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/s4lv0/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "import pre_processing as pp"
   ]
  },
  {
   "source": [
    "## Lettura dataset IMDB avente il seguente formato: review,sentiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset/IMDB.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "source": [
    "## Analisi dataset fornito in input"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimensione dataset:  50000\nSentimenti all'interno del dataset:  ['positive' 'negative']\nNumero di elementi nulli:\n review       0\nsentiment    0\ndtype: int64\nNumero di elementi positivi:  50.0 %\nNumero di elementi negativi:  50.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensione dataset: \", len(dataset))\n",
    "print(\"Sentimenti all'interno del dataset: \", dataset[\"sentiment\"].unique())\n",
    "print(\"Numero di elementi nulli:\\n\", dataset.isnull().sum())\n",
    "print(\"Numero di elementi positivi: \", (len(dataset[\"sentiment\"][dataset.sentiment == \"positive\"])/len(dataset))*100, \"%\")\n",
    "print(\"Numero di elementi negativi: \", (len(dataset[\"sentiment\"][dataset.sentiment == \"negative\"])/len(dataset))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing tweets:: 100%|██████████| 50000/50000 [02:51<00:00, 292.40it/s]\n",
      "word tokenize process: 100%|██████████| 50000/50000 [01:06<00:00, 749.49it/s]\n",
      "Remove stop word: 100%|██████████| 50000/50000 [00:19<00:00, 2578.30it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_review = pp.pre_processing(dataset[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numero di parole uniche: 142092\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero di parole uniche:\", len(set([word for list_word in processed_review for word in list_word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"processed_review\"] = processed_review\n",
    "dataset[\"processed_review\"] = dataset[\"processed_review\"].apply(lambda x: ' '.join(map(str,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = open(\"dataset/dataset_IMDB.pickle\", \"wb\")\n",
    "pickle.dump(dataset, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset/IMDB/dataset_IMDB.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, dataset[\"sentiment\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataset[\"processed_review\"],Y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "source": [
    "Come riportato dall'articolo fornito per il progetto, la fase di weight initialization viene fatta utilizzando il modello pre-addestrato GloVe\n",
    "\n",
    "https://www.aclweb.org/anthology/D14-1162.pdf"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creazione_modello_GloVe(filename):\n",
    "    f = open(filename, encoding=\"utf8\")\n",
    "    embeding_index = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeding_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeding_index"
   ]
  },
  {
   "source": [
    "glove.6B.50d.txt tokenizza le parole in vettore a 50 dimensioni"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = creazione_modello_GloVe(\"dataset/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numero di parole nel modello GloVe:\", len(embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=142092)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "source": [
    "vocab_len = len(word_index)+1\n",
    "embedding_vector_len = embedding[\"banan\"].shape[0]\n",
    "embedding_matrix = np.zeros((vocab_len, embedding_vector_len))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    vector = embedding.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index, :] = vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_vector_len, input_length=200, weights=[embedding_matrix])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "source": [
    "Così come definito dall'articolo, il modello LSTM possiede la seguente architettura: <br>\n",
    "hu=32, batch=64, hl=1, epoch=10, opt=Adam, loss=BCE, out_act=sigmoid, lr=0.001"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 50)           5013850   \n_________________________________________________________________\nlstm (LSTM)                  (None, 32)                10624     \n_________________________________________________________________\ndense (Dense)                (None, 32)                1056      \n_________________________________________________________________\ndense_1 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 5,025,596\nTrainable params: 5,025,596\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(embedding_layer)\n",
    "LSTM_model.add(LSTM(units=32))\n",
    "LSTM_model.add(Dense(32))\n",
    "LSTM_model.add(Dense(2, activation=\"sigmoid\"))\n",
    "\n",
    "#non è necessario modificare l'ottimizzatore di default, poichè già lr=0.001\n",
    "LSTM_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(45000,) (45000,)\n(5000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numero di positivi nel train:  22536\nnumero di negativi nel train:  22464\nnumero di positivi nel test:  2464\nnumero di negativi nel test:  2536\n"
     ]
    }
   ],
   "source": [
    "print(\"numero di positivi nel train: \", list(y_train).count(1))\n",
    "print(\"numero di negativi nel train: \", list(y_train).count(0))\n",
    "\n",
    "print(\"numero di positivi nel test: \", list(y_test).count(1))\n",
    "print(\"numero di negativi nel test: \", list(y_test).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_index = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_index = pad_sequences(x_train_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "547/547 [==============================] - 76s 112ms/step - loss: 0.5316 - accuracy: 0.7183 - val_loss: 0.3259 - val_accuracy: 0.8604\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 56s 103ms/step - loss: 0.2637 - accuracy: 0.8962 - val_loss: 0.2851 - val_accuracy: 0.8799\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.1552 - accuracy: 0.9444 - val_loss: 0.3045 - val_accuracy: 0.8859\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0800 - accuracy: 0.9747 - val_loss: 0.3437 - val_accuracy: 0.8780\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0400 - accuracy: 0.9890 - val_loss: 0.4413 - val_accuracy: 0.8783\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 57s 104ms/step - loss: 0.0184 - accuracy: 0.9952 - val_loss: 0.5329 - val_accuracy: 0.8714\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 57s 103ms/step - loss: 0.0118 - accuracy: 0.9969 - val_loss: 0.7538 - val_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 57s 103ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.7619 - val_accuracy: 0.8659\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 56s 103ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.8399 - val_accuracy: 0.8634\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 56s 103ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.9846 - val_accuracy: 0.8588\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9855c3c2b0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "LSTM_model.fit(x_train_index[:35000], y_train_categorical[:35000], epochs=10, batch_size=64, verbose=1, validation_data=(x_train_index[35000:], y_train_categorical[35000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_index = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_index = pad_sequences(x_test_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = LSTM_model.predict(x_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.84      0.90      0.87      2536\n           1       0.89      0.83      0.86      2464\n\n    accuracy                           0.86      5000\n   macro avg       0.86      0.86      0.86      5000\nweighted avg       0.86      0.86      0.86      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(y_pred, axis=1).astype(\"float32\")))"
   ]
  }
 ]
}