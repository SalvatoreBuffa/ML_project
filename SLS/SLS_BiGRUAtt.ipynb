{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU, Activation, Dropout, Dense, Input, Bidirectional, Layer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import keras.backend as kb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from pre_processing import creazione_modello_GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset/dataset_SLS.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(dataset[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataset[\"sentence\"],Y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2700,) (2700,)\n(300,) (300,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numero di positivi nel train:  1360\nnumero di negativi nel train:  1340\nnumero di positivi nel test:  140\nnumero di negativi nel test:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"numero di positivi nel train: \", list(y_train).count(1))\n",
    "print(\"numero di negativi nel train: \", list(y_train).count(0))\n",
    "\n",
    "print(\"numero di positivi nel test: \", list(y_test).count(1))\n",
    "print(\"numero di negativi nel test: \", list(y_test).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = creazione_modello_GloVe(\"dataset/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=140000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(word_index)+1\n",
    "embedding_vector_len = embedding[\"banana\"].shape[0]\n",
    "embedding_matrix = np.zeros((vocab_len, embedding_vector_len))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    vector = embedding.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index, :] = vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_vector_len, input_length=300, weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=kb.squeeze(kb.tanh(kb.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=kb.softmax(et)\n",
    "        at=kb.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return kb.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_13\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 300, 50)           249800    \n_________________________________________________________________\nbidirectional_13 (Bidirectio (None, 300, 32)           6528      \n_________________________________________________________________\ndense_40 (Dense)             (None, 300, 16)           528       \n_________________________________________________________________\ndense_41 (Dense)             (None, 300, 16)           272       \n_________________________________________________________________\nattention_11 (attention)     (None, 16)                316       \n_________________________________________________________________\ndense_42 (Dense)             (None, 2)                 34        \n=================================================================\nTotal params: 257,478\nTrainable params: 257,478\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BiGRUAtt_model = Sequential()\n",
    "BiGRUAtt_model.add(embedding_layer)\n",
    "BiGRUAtt_model.add(Bidirectional(GRU(units=16, return_sequences=True)))\n",
    "\n",
    "BiGRUAtt_model.add(Dense(16))\n",
    "BiGRUAtt_model.add(Dense(16))\n",
    "\n",
    "BiGRUAtt_model.add(attention())\n",
    "\n",
    "BiGRUAtt_model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "BiGRUAtt_model.compile(loss=\"categorical_crossentropy\", optimizer=Adadelta(), metrics=[\"accuracy\"])\n",
    "BiGRUAtt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_index = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_index = pad_sequences(x_train_index, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "33/33 [==============================] - 10s 223ms/step - loss: 0.6903 - accuracy: 0.5233 - val_loss: 0.6807 - val_accuracy: 0.6117\n",
      "Epoch 2/20\n",
      "33/33 [==============================] - 7s 207ms/step - loss: 0.6507 - accuracy: 0.6367 - val_loss: 0.5888 - val_accuracy: 0.7450\n",
      "Epoch 3/20\n",
      "33/33 [==============================] - 7s 208ms/step - loss: 0.4971 - accuracy: 0.7781 - val_loss: 0.4265 - val_accuracy: 0.8433\n",
      "Epoch 4/20\n",
      "33/33 [==============================] - 7s 217ms/step - loss: 0.3906 - accuracy: 0.8381 - val_loss: 0.3818 - val_accuracy: 0.8483\n",
      "Epoch 5/20\n",
      "33/33 [==============================] - 8s 248ms/step - loss: 0.3100 - accuracy: 0.8771 - val_loss: 0.3420 - val_accuracy: 0.8550\n",
      "Epoch 6/20\n",
      "33/33 [==============================] - 7s 221ms/step - loss: 0.2476 - accuracy: 0.9095 - val_loss: 0.3423 - val_accuracy: 0.8717\n",
      "Epoch 7/20\n",
      "33/33 [==============================] - 7s 198ms/step - loss: 0.1957 - accuracy: 0.9281 - val_loss: 0.3819 - val_accuracy: 0.8533\n",
      "Epoch 8/20\n",
      "33/33 [==============================] - 7s 200ms/step - loss: 0.1536 - accuracy: 0.9486 - val_loss: 0.3499 - val_accuracy: 0.8733\n",
      "Epoch 9/20\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 0.1160 - accuracy: 0.9633 - val_loss: 0.4004 - val_accuracy: 0.8700\n",
      "Epoch 10/20\n",
      "33/33 [==============================] - 7s 199ms/step - loss: 0.0862 - accuracy: 0.9729 - val_loss: 0.4473 - val_accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "33/33 [==============================] - 7s 202ms/step - loss: 0.0651 - accuracy: 0.9795 - val_loss: 0.9612 - val_accuracy: 0.8667\n",
      "Epoch 12/20\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 0.1207 - accuracy: 0.9757 - val_loss: 0.4069 - val_accuracy: 0.8783\n",
      "Epoch 13/20\n",
      "33/33 [==============================] - 7s 201ms/step - loss: 0.0472 - accuracy: 0.9881 - val_loss: 0.6528 - val_accuracy: 0.8667\n",
      "Epoch 14/20\n",
      "33/33 [==============================] - 7s 202ms/step - loss: 0.0281 - accuracy: 0.9943 - val_loss: 0.9213 - val_accuracy: 0.8667\n",
      "Epoch 15/20\n",
      "33/33 [==============================] - 7s 212ms/step - loss: 0.0400 - accuracy: 0.9914 - val_loss: 0.7138 - val_accuracy: 0.8583\n",
      "Epoch 16/20\n",
      "33/33 [==============================] - 7s 211ms/step - loss: 0.0380 - accuracy: 0.9905 - val_loss: 0.7580 - val_accuracy: 0.8617\n",
      "Epoch 17/20\n",
      "33/33 [==============================] - 8s 236ms/step - loss: 0.0193 - accuracy: 0.9967 - val_loss: 0.8679 - val_accuracy: 0.8600\n",
      "Epoch 18/20\n",
      "33/33 [==============================] - 9s 280ms/step - loss: 0.0165 - accuracy: 0.9971 - val_loss: 0.9256 - val_accuracy: 0.8633\n",
      "Epoch 19/20\n",
      "33/33 [==============================] - 8s 244ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.9943 - val_accuracy: 0.8633\n",
      "Epoch 20/20\n",
      "33/33 [==============================] - 7s 207ms/step - loss: 0.0136 - accuracy: 0.9976 - val_loss: 1.0454 - val_accuracy: 0.8650\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f90753f2be0>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "BiGRUAtt_model.fit(x_train_index[:2100], y_train_categorical[:2100], epochs=20, batch_size=64, verbose=1, validation_data=(x_train_index[2100:], y_train_categorical[2100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_index = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_index = pad_sequences(x_test_index, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = BiGRUAtt_model.predict(x_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.90      0.82      0.86       160\n           1       0.82      0.89      0.85       140\n\n    accuracy                           0.86       300\n   macro avg       0.86      0.86      0.86       300\nweighted avg       0.86      0.86      0.86       300\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(y_pred, axis=1).astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_37_layer_call_fn, gru_cell_37_layer_call_and_return_conditional_losses, gru_cell_38_layer_call_fn, gru_cell_38_layer_call_and_return_conditional_losses, gru_cell_37_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: models/BiGRUAtt/assets\n",
      "INFO:tensorflow:Assets written to: models/BiGRUAtt/assets\n"
     ]
    }
   ],
   "source": [
    "BiGRUAtt_model.save(\"models/BiGRUAtt\")"
   ]
  }
 ]
}