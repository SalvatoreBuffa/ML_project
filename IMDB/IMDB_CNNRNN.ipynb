{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Activation, Dropout, Dense, Conv1D, GlobalMaxPool1D, MaxPool1D, Input, MaxPooling1D, Flatten, SimpleRNN\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "from tensorflow.keras.activations import tanh\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset/dataset_IMDB.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, dataset[\"sentiment\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataset[\"processed_review\"],Y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numero di positivi nel train:  22477\nnumero di negativi nel train:  22523\nnumero di positivi nel test:  2523\nnumero di negativi nel test:  2477\n"
     ]
    }
   ],
   "source": [
    "print(\"numero di positivi nel train: \", list(y_train).count(1))\n",
    "print(\"numero di negativi nel train: \", list(y_train).count(0))\n",
    "\n",
    "print(\"numero di positivi nel test: \", list(y_test).count(1))\n",
    "print(\"numero di negativi nel test: \", list(y_test).count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creazione_modello_GloVe(filename):\n",
    "    f = open(filename, encoding=\"utf8\")\n",
    "    embeding_index = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeding_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = creazione_modello_GloVe(\"dataset/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=142092)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(word_index)+1\n",
    "embedding_vector_len = embedding[\"banan\"].shape[0]\n",
    "embedding_matrix = np.zeros((vocab_len, embedding_vector_len))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    vector = embedding.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index, :] = vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_vector_len, input_length=200, weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 50)           5017150   \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 200, 4)            1604      \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 40, 4)             0         \n_________________________________________________________________\nsimple_rnn_2 (SimpleRNN)     (None, 64)                4416      \n_________________________________________________________________\ndense_6 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                4160      \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 130       \n=================================================================\nTotal params: 5,031,620\nTrainable params: 5,031,620\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "CNNRNN_model = Sequential()\n",
    "CNNRNN_model.add(embedding_layer)\n",
    "#CNN\n",
    "CNNRNN_model.add(Conv1D(filters=4, kernel_size=8, padding=\"same\"))\n",
    "CNNRNN_model.add(MaxPooling1D(5, padding=\"same\"))\n",
    "#CNNRNN_model.add(Flatten())\n",
    "#RNN\n",
    "CNNRNN_model.add(SimpleRNN(64))\n",
    "CNNRNN_model.add(Dense(64))\n",
    "CNNRNN_model.add(Dense(64))\n",
    "#CNN_model.add(Dropout(0.2))\n",
    "#CNN_model.add(Dense(20))\n",
    "CNNRNN_model.add(Dense(2, activation=tanh))\n",
    "#Adagrad(learning_rate=0.15)\n",
    "CNNRNN_model.compile(loss=\"binary_crossentropy\", optimizer=Adagrad(learning_rate=0.15), metrics=[\"accuracy\"])\n",
    "CNNRNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_index = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_index = pad_sequences(x_train_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "547/547 [==============================] - 34s 60ms/step - loss: 0.4805 - accuracy: 0.7678 - val_loss: 0.3736 - val_accuracy: 0.8307\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 29s 52ms/step - loss: 0.2917 - accuracy: 0.8842 - val_loss: 0.3223 - val_accuracy: 0.8680\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 28s 52ms/step - loss: 0.1727 - accuracy: 0.9373 - val_loss: 0.3049 - val_accuracy: 0.8831\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 29s 52ms/step - loss: 0.0879 - accuracy: 0.9722 - val_loss: 0.4195 - val_accuracy: 0.8712\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 29s 52ms/step - loss: 0.0492 - accuracy: 0.9855 - val_loss: 0.4517 - val_accuracy: 0.8739\n",
      "Epoch 6/10\n",
      "547/547 [==============================] - 29s 53ms/step - loss: 0.0311 - accuracy: 0.9903 - val_loss: 0.5332 - val_accuracy: 0.8694\n",
      "Epoch 7/10\n",
      "547/547 [==============================] - 29s 53ms/step - loss: 0.0317 - accuracy: 0.9895 - val_loss: 0.5761 - val_accuracy: 0.8611\n",
      "Epoch 8/10\n",
      "547/547 [==============================] - 29s 53ms/step - loss: 0.0162 - accuracy: 0.9953 - val_loss: 0.6226 - val_accuracy: 0.8720\n",
      "Epoch 9/10\n",
      "547/547 [==============================] - 29s 53ms/step - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.8400 - val_accuracy: 0.8729\n",
      "Epoch 10/10\n",
      "547/547 [==============================] - 29s 53ms/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.8095 - val_accuracy: 0.8585\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f85540f1970>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "clear_session()\n",
    "CNNRNN_model.fit(x_train_index[:35000], y_train_categorical[:35000], epochs=10, batch_size=64, verbose=1, validation_data=(x_train_index[35000:], y_train_categorical[35000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_index = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_index = pad_sequences(x_test_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CNNRNN_model.predict(x_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.83      0.88      0.86      2477\n           1       0.87      0.83      0.85      2523\n\n    accuracy                           0.85      5000\n   macro avg       0.85      0.85      0.85      5000\nweighted avg       0.85      0.85      0.85      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(y_pred, axis=1).astype(\"float32\")))"
   ]
  }
 ]
}