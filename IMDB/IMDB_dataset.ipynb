{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/s4lv0/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/s4lv0/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model, Sequential\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pre_processing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset/IMDB.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimensione dataset:  50000\nSentimenti all'interno del dataset:  ['positive' 'negative']\nNumero di elementi nulli:\n review       0\nsentiment    0\ndtype: int64\nNumero di elementi positivi:  50.0 %\nNumero di elementi negativi:  50.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensione dataset: \", len(dataset))\n",
    "print(\"Sentimenti all'interno del dataset: \", dataset[\"sentiment\"].unique())\n",
    "print(\"Numero di elementi nulli:\\n\", dataset.isnull().sum())\n",
    "print(\"Numero di elementi positivi: \", (len(dataset[\"sentiment\"][dataset.sentiment == \"positive\"])/len(dataset))*100, \"%\")\n",
    "print(\"Numero di elementi negativi: \", (len(dataset[\"sentiment\"][dataset.sentiment == \"negative\"])/len(dataset))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processing tweets:: 100%|██████████| 50000/50000 [02:50<00:00, 293.66it/s]\n",
      "word tokenize process: 100%|██████████| 50000/50000 [01:06<00:00, 755.50it/s]\n",
      "Remove stop word: 100%|██████████| 50000/50000 [00:19<00:00, 2606.78it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_review = pp.pre_processing(dataset[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numero di parole uniche: 142092\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero di parole uniche:\", len(set([word for list_word in processed_review for word in list_word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"processed_review\"] = processed_review\n",
    "dataset[\"processed_review\"] = dataset[\"processed_review\"].apply(lambda x: ' '.join(map(str,x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = open(\"dataset/dataset_IMDB.pickle\", \"wb\")\n",
    "pickle.dump(dataset, files)"
   ]
  }
 ]
}