{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "799eb0fd45bbf32cd22b7eb12315f66a6945dddec75ab4ee4de6611b037c6c79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GRU, Activation, Dropout, Dense, Input, Bidirectional, Layer\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import keras.backend as kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"dataset/IMDB/dataset_IMDB.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, dataset[\"sentiment\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(dataset[\"processed_review\"],Y, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creazione_modello_GloVe(filename):\n",
    "    f = open(filename, encoding=\"utf8\")\n",
    "    embeding_index = {}\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embeding_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeding_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = creazione_modello_GloVe(\"dataset/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=142092)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(word_index)+1\n",
    "embedding_vector_len = embedding[\"banan\"].shape[0]\n",
    "embedding_matrix = np.zeros((vocab_len, embedding_vector_len))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    vector = embedding.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index, :] = vector\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_vector_len, input_length=200, weights=[embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
    "        super(attention, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        et=kb.squeeze(kb.tanh(kb.dot(x,self.W)+self.b),axis=-1)\n",
    "        at=kb.softmax(et)\n",
    "        at=kb.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return kb.sum(output,axis=1)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 200, 50)           5008200   \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 200, 64)           16128     \n_________________________________________________________________\ndense_6 (Dense)              (None, 200, 32)           2080      \n_________________________________________________________________\ndense_7 (Dense)              (None, 200, 32)           1056      \n_________________________________________________________________\nattention_2 (attention)      (None, 32)                232       \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 66        \n=================================================================\nTotal params: 5,027,762\nTrainable params: 5,027,762\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BiGRUAtt_model = Sequential()\n",
    "BiGRUAtt_model.add(embedding_layer)\n",
    "BiGRUAtt_model.add(Bidirectional(GRU(units=32, return_sequences=True)))\n",
    "BiGRUAtt_model.add(Dense(32))\n",
    "BiGRUAtt_model.add(Dense(32))\n",
    "BiGRUAtt_model.add(attention())\n",
    "BiGRUAtt_model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "BiGRUAtt_model.compile(loss=\"categorical_crossentropy\", optimizer=Adagrad(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "BiGRUAtt_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_index = tokenizer.texts_to_sequences(x_train)\n",
    "x_train_index = pad_sequences(x_train_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_categorical = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "274/274 [==============================] - 48s 165ms/step - loss: 0.6695 - accuracy: 0.6167 - val_loss: 0.6301 - val_accuracy: 0.7066\n",
      "Epoch 2/30\n",
      "274/274 [==============================] - 46s 169ms/step - loss: 0.5833 - accuracy: 0.7183 - val_loss: 0.5614 - val_accuracy: 0.7143\n",
      "Epoch 3/30\n",
      "274/274 [==============================] - 47s 172ms/step - loss: 0.5259 - accuracy: 0.7483 - val_loss: 0.5066 - val_accuracy: 0.7566\n",
      "Epoch 4/30\n",
      "274/274 [==============================] - 48s 176ms/step - loss: 0.4973 - accuracy: 0.7654 - val_loss: 0.4861 - val_accuracy: 0.7686\n",
      "Epoch 5/30\n",
      "274/274 [==============================] - 48s 176ms/step - loss: 0.4781 - accuracy: 0.7755 - val_loss: 0.4718 - val_accuracy: 0.7783\n",
      "Epoch 6/30\n",
      "274/274 [==============================] - 49s 177ms/step - loss: 0.4600 - accuracy: 0.7867 - val_loss: 0.4618 - val_accuracy: 0.7796\n",
      "Epoch 7/30\n",
      "274/274 [==============================] - 49s 178ms/step - loss: 0.4444 - accuracy: 0.7976 - val_loss: 0.4357 - val_accuracy: 0.8017\n",
      "Epoch 8/30\n",
      "274/274 [==============================] - 49s 180ms/step - loss: 0.4308 - accuracy: 0.8044 - val_loss: 0.4291 - val_accuracy: 0.7995\n",
      "Epoch 9/30\n",
      "274/274 [==============================] - 49s 181ms/step - loss: 0.4205 - accuracy: 0.8114 - val_loss: 0.4211 - val_accuracy: 0.8061\n",
      "Epoch 10/30\n",
      "274/274 [==============================] - 50s 181ms/step - loss: 0.4115 - accuracy: 0.8167 - val_loss: 0.4049 - val_accuracy: 0.8186\n",
      "Epoch 11/30\n",
      "274/274 [==============================] - 50s 183ms/step - loss: 0.4036 - accuracy: 0.8203 - val_loss: 0.4000 - val_accuracy: 0.8224\n",
      "Epoch 12/30\n",
      "274/274 [==============================] - 50s 184ms/step - loss: 0.3968 - accuracy: 0.8232 - val_loss: 0.4029 - val_accuracy: 0.8206\n",
      "Epoch 13/30\n",
      "274/274 [==============================] - 51s 184ms/step - loss: 0.3912 - accuracy: 0.8277 - val_loss: 0.3935 - val_accuracy: 0.8270\n",
      "Epoch 14/30\n",
      "274/274 [==============================] - 51s 185ms/step - loss: 0.3842 - accuracy: 0.8313 - val_loss: 0.3887 - val_accuracy: 0.8294\n",
      "Epoch 15/30\n",
      "274/274 [==============================] - 51s 186ms/step - loss: 0.3798 - accuracy: 0.8326 - val_loss: 0.3773 - val_accuracy: 0.8310\n",
      "Epoch 16/30\n",
      "274/274 [==============================] - 51s 186ms/step - loss: 0.3729 - accuracy: 0.8377 - val_loss: 0.3798 - val_accuracy: 0.8348\n",
      "Epoch 17/30\n",
      "274/274 [==============================] - 51s 188ms/step - loss: 0.3687 - accuracy: 0.8392 - val_loss: 0.3757 - val_accuracy: 0.8367\n",
      "Epoch 18/30\n",
      "274/274 [==============================] - 49s 181ms/step - loss: 0.3644 - accuracy: 0.8425 - val_loss: 0.3622 - val_accuracy: 0.8411\n",
      "Epoch 19/30\n",
      "274/274 [==============================] - 47s 172ms/step - loss: 0.3589 - accuracy: 0.8443 - val_loss: 0.3599 - val_accuracy: 0.8425\n",
      "Epoch 20/30\n",
      "274/274 [==============================] - 47s 172ms/step - loss: 0.3554 - accuracy: 0.8465 - val_loss: 0.3562 - val_accuracy: 0.8440\n",
      "Epoch 21/30\n",
      "274/274 [==============================] - 49s 178ms/step - loss: 0.3515 - accuracy: 0.8490 - val_loss: 0.3633 - val_accuracy: 0.8435\n",
      "Epoch 22/30\n",
      "274/274 [==============================] - 48s 175ms/step - loss: 0.3483 - accuracy: 0.8509 - val_loss: 0.3504 - val_accuracy: 0.8468\n",
      "Epoch 23/30\n",
      "274/274 [==============================] - 48s 175ms/step - loss: 0.3439 - accuracy: 0.8523 - val_loss: 0.3448 - val_accuracy: 0.8515\n",
      "Epoch 24/30\n",
      "274/274 [==============================] - 48s 175ms/step - loss: 0.3409 - accuracy: 0.8548 - val_loss: 0.3433 - val_accuracy: 0.8503\n",
      "Epoch 25/30\n",
      "274/274 [==============================] - 48s 176ms/step - loss: 0.3375 - accuracy: 0.8557 - val_loss: 0.3441 - val_accuracy: 0.8491\n",
      "Epoch 26/30\n",
      "274/274 [==============================] - 50s 181ms/step - loss: 0.3344 - accuracy: 0.8570 - val_loss: 0.3373 - val_accuracy: 0.8569\n",
      "Epoch 27/30\n",
      "274/274 [==============================] - 49s 180ms/step - loss: 0.3311 - accuracy: 0.8595 - val_loss: 0.3347 - val_accuracy: 0.8596\n",
      "Epoch 28/30\n",
      "274/274 [==============================] - 37s 136ms/step - loss: 0.3284 - accuracy: 0.8611 - val_loss: 0.3370 - val_accuracy: 0.8536\n",
      "Epoch 29/30\n",
      "274/274 [==============================] - 31s 115ms/step - loss: 0.3256 - accuracy: 0.8623 - val_loss: 0.3351 - val_accuracy: 0.8538\n",
      "Epoch 30/30\n",
      "274/274 [==============================] - 32s 115ms/step - loss: 0.3233 - accuracy: 0.8627 - val_loss: 0.3293 - val_accuracy: 0.8591\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff8f0350a60>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "BiGRUAtt_model.fit(x_train_index[:35000], y_train_categorical[:35000], epochs=30, batch_size=128, verbose=1, validation_data=(x_train_index[35000:], y_train_categorical[35000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_index = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_index = pad_sequences(x_test_index, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = BiGRUAtt_model.predict(x_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.87      0.83      0.85      2451\n           1       0.85      0.88      0.86      2549\n\n    accuracy                           0.86      5000\n   macro avg       0.86      0.86      0.86      5000\nweighted avg       0.86      0.86      0.86      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, np.argmax(y_pred, axis=1).astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_7_layer_call_and_return_conditional_losses, gru_cell_7_layer_call_fn, gru_cell_8_layer_call_and_return_conditional_losses, gru_cell_8_layer_call_fn, gru_cell_7_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "INFO:tensorflow:Assets written to: IMDB_BiGRUAtt/assets\n",
      "INFO:tensorflow:Assets written to: IMDB_BiGRUAtt/assets\n"
     ]
    }
   ],
   "source": [
    "BiGRUAtt_model.save(\"IMDB_BiGRUAtt\")"
   ]
  }
 ]
}